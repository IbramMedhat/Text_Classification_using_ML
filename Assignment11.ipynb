{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20newsgroups dataset (A collection of 20,000 news items across 20 categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(subset=\"all\", random_state=3116 , download_if_missing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the dataset to only the following two categories named as ’sci.med’ and ’comp.graphics’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_boolean_mask_1 = dataset.target == 1\n",
    "dataset_boolean_mask_2 = dataset.target == 13\n",
    "dataset_boolean_mask = dataset_boolean_mask_1 | dataset_boolean_mask_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can that the needed target values for this task are 1, 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_data = [data_value for data_value, mask_value in zip(dataset.data, dataset_boolean_mask) if mask_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_target = dataset.target[dataset_boolean_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Preprocessing textual data to remove punctuation, stop-words (list available via external libraries\n",
    "such as NLTK and spaCy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First getting the list of english language stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "dataset_data_no_stopwords = []\n",
    "\n",
    "for news_index in range(len(dataset_data)) :\n",
    "    filtered_news = []\n",
    "    current_news = dataset_data[news_index]\n",
    "    #Filtering out punctuation\n",
    "    punctuation_removing_tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    word_tokens = punctuation_removing_tokenizer.tokenize(current_news)\n",
    "    for word in word_tokens :\n",
    "        if word not in stop_words :\n",
    "            filtered_news.append(word)\n",
    "    dataset_data_no_stopwords.append(filtered_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Implementing a bag-of-words feature representation for each text sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From',\n",
       " 'ame_0123',\n",
       " 'bigdog',\n",
       " 'engr',\n",
       " 'arizona',\n",
       " 'edu',\n",
       " 'Terrance',\n",
       " 'J',\n",
       " 'Dishongh',\n",
       " 'Subject',\n",
       " 'Strain',\n",
       " 'Gage',\n",
       " 'Applications',\n",
       " 'vivo',\n",
       " 'Organization',\n",
       " 'University',\n",
       " 'Arizona',\n",
       " 'Lines',\n",
       " '14',\n",
       " 'Greeting',\n",
       " 'I',\n",
       " 'starting',\n",
       " 'work',\n",
       " 'project',\n",
       " 'trying',\n",
       " 'make',\n",
       " 'strain',\n",
       " 'gages',\n",
       " 'bond',\n",
       " 'bone',\n",
       " 'period',\n",
       " 'several',\n",
       " 'months',\n",
       " 'currently',\n",
       " 'using',\n",
       " 'hydroxyapaptite',\n",
       " 'back',\n",
       " 'tried',\n",
       " 'M',\n",
       " 'bonding',\n",
       " 'Apart',\n",
       " 'two',\n",
       " 'application',\n",
       " 'methods',\n",
       " 'seem',\n",
       " 'much',\n",
       " 'else',\n",
       " 'literature',\n",
       " 'engineering',\n",
       " 'background',\n",
       " 'medical',\n",
       " 'biological',\n",
       " 'would',\n",
       " 'interest',\n",
       " 'ideas',\n",
       " 'stimulte',\n",
       " 'growth',\n",
       " 'surface',\n",
       " 'cortical',\n",
       " 'Thanks',\n",
       " 'oyur',\n",
       " 'help',\n",
       " 'Advance',\n",
       " 'zyeh',\n",
       " 'caspian',\n",
       " 'usc',\n",
       " 'zhenghao',\n",
       " 'yeh',\n",
       " 'Re',\n",
       " 'Need',\n",
       " 'polygon',\n",
       " 'splitting',\n",
       " 'algo',\n",
       " 'Southern',\n",
       " 'California',\n",
       " 'Los',\n",
       " 'Angeles',\n",
       " 'CA',\n",
       " '25',\n",
       " 'Distribution',\n",
       " 'world',\n",
       " 'NNTP',\n",
       " 'Posting',\n",
       " 'Host',\n",
       " 'Keywords',\n",
       " 'polygons',\n",
       " 'clipping',\n",
       " 'In',\n",
       " 'article',\n",
       " '1qvq4b',\n",
       " 'r4t',\n",
       " 'wampyr',\n",
       " 'cc',\n",
       " 'uow',\n",
       " 'au',\n",
       " 'g9134255',\n",
       " 'Coronado',\n",
       " 'Emmanuel',\n",
       " 'Abad',\n",
       " 'writes',\n",
       " 'The',\n",
       " 'idea',\n",
       " 'clip',\n",
       " 'one',\n",
       " 'another',\n",
       " 'necessarily',\n",
       " 'rectangular',\n",
       " 'window',\n",
       " 'My',\n",
       " 'problem',\n",
       " 'finding',\n",
       " 'new',\n",
       " 'vertices',\n",
       " 'resulting',\n",
       " 'subpolygons',\n",
       " 'first',\n",
       " 'Is',\n",
       " 'simply',\n",
       " 'matter',\n",
       " 'extending',\n",
       " 'usual',\n",
       " 'algorithm',\n",
       " 'whereby',\n",
       " 'edges',\n",
       " 'checked',\n",
       " 'simpler',\n",
       " 'way',\n",
       " 'Comments',\n",
       " 'welcome',\n",
       " 'Noel',\n",
       " 'It',\n",
       " 'depends',\n",
       " 'kind',\n",
       " 'Convex',\n",
       " 'simple',\n",
       " 'concave',\n",
       " 'trouble',\n",
       " 'loop',\n",
       " 'inside',\n",
       " 'big',\n",
       " 'Of',\n",
       " 'cause',\n",
       " 'use',\n",
       " 'box',\n",
       " 'test',\n",
       " 'avoid',\n",
       " 'checking',\n",
       " 'According',\n",
       " 'experience',\n",
       " 'go',\n",
       " 'headache',\n",
       " 'stuff',\n",
       " 'deal',\n",
       " 'special',\n",
       " 'cases',\n",
       " 'example',\n",
       " 'overlapped',\n",
       " 'lines',\n",
       " 'Yeh',\n",
       " 'USC',\n",
       " 'rind',\n",
       " 'enterprise',\n",
       " 'bih',\n",
       " 'harvard',\n",
       " 'David',\n",
       " 'Rind',\n",
       " 'Candida',\n",
       " 'yeast',\n",
       " 'Bloom',\n",
       " 'Fact',\n",
       " 'Fiction',\n",
       " 'Beth',\n",
       " 'Israel',\n",
       " 'Hospital',\n",
       " 'Harvard',\n",
       " 'Medical',\n",
       " 'School',\n",
       " 'Boston',\n",
       " 'Mass',\n",
       " 'USA',\n",
       " '18',\n",
       " '1993Apr23',\n",
       " '180430',\n",
       " '1',\n",
       " 'vms',\n",
       " 'ocom',\n",
       " 'okstate',\n",
       " 'banschbach',\n",
       " 'like',\n",
       " 'term',\n",
       " 'quack',\n",
       " 'applied',\n",
       " 'licensed',\n",
       " 'physician',\n",
       " 'Questionable',\n",
       " 'conduct',\n",
       " 'appropriately',\n",
       " 'called',\n",
       " 'unethical',\n",
       " 'opinion',\n",
       " '3',\n",
       " 'Using',\n",
       " 'laetril',\n",
       " 'treat',\n",
       " 'cancer',\n",
       " 'patients',\n",
       " 'treatment',\n",
       " 'shown',\n",
       " 'ineffective',\n",
       " 'dangerous',\n",
       " 'cyanide',\n",
       " 'release',\n",
       " 'NCI',\n",
       " 'Hmm',\n",
       " 'This',\n",
       " 'certainly',\n",
       " 'among',\n",
       " 'things',\n",
       " 'refer',\n",
       " 'therapy',\n",
       " 'tend',\n",
       " 'practitioner',\n",
       " 'prescribed',\n",
       " 'laetrile',\n",
       " 'whether',\n",
       " 'There',\n",
       " 'behaviors',\n",
       " 'ordering',\n",
       " 'unneccessary',\n",
       " 'tests',\n",
       " 'increase',\n",
       " 'fees',\n",
       " 'lable',\n",
       " 'quackish',\n",
       " 'prescribing',\n",
       " 'known',\n",
       " 'therapies',\n",
       " 'seems',\n",
       " 'hallmarks',\n",
       " 'State',\n",
       " 'ICGLN',\n",
       " 'ASUACAD',\n",
       " 'BITNET',\n",
       " 'Burzynski',\n",
       " 'Antineoplastons',\n",
       " '93111',\n",
       " '145432ICGLN',\n",
       " 'C6BJyt',\n",
       " 'A1K',\n",
       " 'ssr',\n",
       " 'com',\n",
       " '37',\n",
       " 'nnget',\n",
       " '93122',\n",
       " '1300541',\n",
       " 'sdb',\n",
       " 'Scott',\n",
       " 'Ballantyne',\n",
       " 'says',\n",
       " 'Moss',\n",
       " 'People',\n",
       " 'Against',\n",
       " 'Cancer',\n",
       " 'Director',\n",
       " 'Communications',\n",
       " 'offer',\n",
       " 'pretty',\n",
       " 'questionable',\n",
       " 'information',\n",
       " 'exactly',\n",
       " 'place',\n",
       " 'patient',\n",
       " 'advised',\n",
       " 'turn',\n",
       " 'And',\n",
       " 'advise',\n",
       " 'people',\n",
       " 'Most',\n",
       " 'maybe',\n",
       " 'infomation',\n",
       " 'latest',\n",
       " 'catalogue',\n",
       " 'concern',\n",
       " 'treatments',\n",
       " 'many',\n",
       " 'quite',\n",
       " 'well',\n",
       " 'offered',\n",
       " 'circular',\n",
       " 'refutation',\n",
       " 'organization',\n",
       " 'Who',\n",
       " 'book',\n",
       " 'PAC',\n",
       " 'Could',\n",
       " 'regulatory',\n",
       " 'agencies',\n",
       " 'industries',\n",
       " 'showing',\n",
       " 'operating',\n",
       " 'major',\n",
       " 'vested',\n",
       " 'interests',\n",
       " 'Whether',\n",
       " 'believes',\n",
       " 'real',\n",
       " 'actually',\n",
       " 'shape',\n",
       " 'research',\n",
       " 'seperate',\n",
       " 'argument',\n",
       " 'If',\n",
       " 'sees',\n",
       " 'possibility',\n",
       " 'however',\n",
       " 'exist',\n",
       " 'fact',\n",
       " 'put',\n",
       " 'refuted',\n",
       " 'industry',\n",
       " 'hold',\n",
       " 'weight',\n",
       " 'As',\n",
       " 'ineffectiveness',\n",
       " 'antineoplasteons',\n",
       " 'NIH',\n",
       " 'find',\n",
       " 'effective',\n",
       " 'sense',\n",
       " 'course',\n",
       " 'faith',\n",
       " 'word',\n",
       " 'alive',\n",
       " 'told',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'dead',\n",
       " 'soon',\n",
       " 'They',\n",
       " 'fighting',\n",
       " 'hell',\n",
       " 'keep',\n",
       " 'clinic',\n",
       " 'open',\n",
       " 'credit',\n",
       " 'survival',\n",
       " 'Anyone',\n",
       " 'looks',\n",
       " 'record',\n",
       " 'investigation',\n",
       " 'alterna',\n",
       " 'tive',\n",
       " 'easily',\n",
       " 'see',\n",
       " 'strange',\n",
       " 'knack',\n",
       " 'ing',\n",
       " 'relatively',\n",
       " 'cheap',\n",
       " 'nontoxic',\n",
       " 'useless',\n",
       " 'gn',\n",
       " 'pkhalsa',\n",
       " 'wpi',\n",
       " 'WPI',\n",
       " 'EDU',\n",
       " 'Partap',\n",
       " 'S',\n",
       " 'Khalsa',\n",
       " 'Worcester',\n",
       " 'Polytechnic',\n",
       " 'Institute',\n",
       " '27',\n",
       " 'inet',\n",
       " '1993Apr28',\n",
       " '173600',\n",
       " '21703',\n",
       " 'organpipe',\n",
       " 'uug',\n",
       " 'good',\n",
       " 'entitled',\n",
       " 'A',\n",
       " 'long',\n",
       " 'measurement',\n",
       " 'device',\n",
       " 'Journal',\n",
       " 'Investigative',\n",
       " 'Surgery',\n",
       " '1989',\n",
       " '2',\n",
       " '195',\n",
       " '206',\n",
       " 'Szivek',\n",
       " 'JA',\n",
       " 'Magee',\n",
       " 'FP',\n",
       " 'think',\n",
       " 'others',\n",
       " 'searching',\n",
       " 'MedLine',\n",
       " 'MS',\n",
       " 'DC',\n",
       " 'FACO',\n",
       " 'Post',\n",
       " 'Doc',\n",
       " 'Research',\n",
       " 'Fellow',\n",
       " 'U',\n",
       " 'Med',\n",
       " 'antonio',\n",
       " 'qualcom',\n",
       " 'qualcomm',\n",
       " 'Franklin',\n",
       " 'Antonio',\n",
       " 'Thermoscan',\n",
       " 'ear',\n",
       " 'thermometer',\n",
       " 'Nntp',\n",
       " 'Qualcomm',\n",
       " 'Inc',\n",
       " 'San',\n",
       " 'Diego',\n",
       " '39',\n",
       " 'ASHWIN',\n",
       " '93May1225032',\n",
       " 'leo',\n",
       " 'gatech',\n",
       " 'ashwin',\n",
       " 'Ashwin',\n",
       " 'Ram',\n",
       " 'Does',\n",
       " 'instrument',\n",
       " 'really',\n",
       " 'supposed',\n",
       " 'give',\n",
       " 'fast',\n",
       " 'accurate',\n",
       " 'temperature',\n",
       " 'reading',\n",
       " 'How',\n",
       " 'far',\n",
       " 'insert',\n",
       " 'worth',\n",
       " '100',\n",
       " 'selling',\n",
       " 'No',\n",
       " 'doctor',\n",
       " 'started',\n",
       " 'recently',\n",
       " 'thought',\n",
       " 'concept',\n",
       " 'amazing',\n",
       " 'bought',\n",
       " 'thing',\n",
       " 'works',\n",
       " 'infrared',\n",
       " 'emissions',\n",
       " 'drum',\n",
       " 'hotter',\n",
       " 'canal',\n",
       " 'walls',\n",
       " 'point',\n",
       " 'carefully',\n",
       " 'means',\n",
       " 'tugging',\n",
       " 'top',\n",
       " 'straighten',\n",
       " 'inserting',\n",
       " 'snugly',\n",
       " 'pushing',\n",
       " 'button',\n",
       " 'Unfortunately',\n",
       " 'wrong',\n",
       " 'almost',\n",
       " 'impossible',\n",
       " 'aim',\n",
       " 'correctly',\n",
       " 'get',\n",
       " 'readings',\n",
       " 'differ',\n",
       " 'degrees',\n",
       " 'may',\n",
       " 'oral',\n",
       " 'talked',\n",
       " 'nurses',\n",
       " 'office',\n",
       " 'said',\n",
       " 'either',\n",
       " 'reasons',\n",
       " 'She',\n",
       " 'instruction',\n",
       " 'tug',\n",
       " 'correct',\n",
       " 'insertion',\n",
       " 'feels',\n",
       " 'self',\n",
       " 'also',\n",
       " 'complained',\n",
       " 'company',\n",
       " 'inaccurate',\n",
       " 'someone',\n",
       " 'take',\n",
       " 'great',\n",
       " 'care',\n",
       " 'clean',\n",
       " 'end',\n",
       " 'probe',\n",
       " 'alcohol',\n",
       " 'time',\n",
       " 'demonstrated',\n",
       " 'prior',\n",
       " 'managed',\n",
       " 'within',\n",
       " '0',\n",
       " '5',\n",
       " 'degree',\n",
       " 'took',\n",
       " 'home',\n",
       " 'driving',\n",
       " 'Dr',\n",
       " 'noticed',\n",
       " 'tha',\n",
       " 'click',\n",
       " 'remove',\n",
       " 'immediately',\n",
       " 'causes',\n",
       " 'leave',\n",
       " 'seconds',\n",
       " 'clicking',\n",
       " 'nurse',\n",
       " 'agreed',\n",
       " 'suspect',\n",
       " 'realize',\n",
       " 'therefore',\n",
       " 'bad',\n",
       " 'yet',\n",
       " 'reason',\n",
       " 'short',\n",
       " 'folks',\n",
       " 'believe',\n",
       " 'person',\n",
       " 'wants',\n",
       " 'gcdcrgm',\n",
       " 'state',\n",
       " 'systems',\n",
       " 'sa',\n",
       " 'gov',\n",
       " 'PICLAB',\n",
       " 'processing',\n",
       " 'half',\n",
       " 'GIF',\n",
       " 'Systems',\n",
       " 'South',\n",
       " 'Australia',\n",
       " '6',\n",
       " 'playing',\n",
       " 'program',\n",
       " 'modify',\n",
       " 'gif',\n",
       " 'files',\n",
       " 'keeps',\n",
       " 'displaying',\n",
       " '50',\n",
       " 'image',\n",
       " 'Starting',\n",
       " 'displays20',\n",
       " 'leaves',\n",
       " '20',\n",
       " 'blank',\n",
       " 'etc',\n",
       " 'ANyone',\n",
       " 'know',\n",
       " 'piece',\n",
       " 'software',\n",
       " 'instead',\n",
       " 'tolerance',\n",
       " 'teckjoo',\n",
       " 'iti',\n",
       " 'sg',\n",
       " 'Chua',\n",
       " 'Teck',\n",
       " 'Joo',\n",
       " 'Visuallib',\n",
       " '3D',\n",
       " 'graphics',\n",
       " 'Windows',\n",
       " 'Information',\n",
       " 'Technology',\n",
       " 'National',\n",
       " 'Computer',\n",
       " 'Board',\n",
       " 'Singapore',\n",
       " '17',\n",
       " 'looking',\n",
       " 'library',\n",
       " 'runs',\n",
       " 'Are',\n",
       " 'libraries',\n",
       " 'must',\n",
       " 'run',\n",
       " 'VGA',\n",
       " 'require',\n",
       " 'add',\n",
       " 'cards',\n",
       " 'For',\n",
       " 'Metaware',\n",
       " 'High',\n",
       " 'C',\n",
       " 'compiler',\n",
       " 'v3',\n",
       " 'Any',\n",
       " 'email',\n",
       " 'contact',\n",
       " 'author',\n",
       " 'appreciated',\n",
       " 'Email',\n",
       " '71',\n",
       " 'Science',\n",
       " 'Park',\n",
       " 'Drive',\n",
       " 'Phone',\n",
       " '65',\n",
       " '772',\n",
       " '0237',\n",
       " '0511',\n",
       " 'Fax',\n",
       " '779',\n",
       " '1827',\n",
       " 'Looking',\n",
       " 'Tseng',\n",
       " 'VESA',\n",
       " 'drivers',\n",
       " 't890449',\n",
       " 'patan',\n",
       " 'fi',\n",
       " 'upm',\n",
       " 'es',\n",
       " 'usr',\n",
       " 'local',\n",
       " 'lib',\n",
       " '10',\n",
       " 'Hi',\n",
       " 'msg',\n",
       " 'Net',\n",
       " '3rd',\n",
       " 'copy',\n",
       " 'dam',\n",
       " 'ed',\n",
       " 'VI',\n",
       " 'Look',\n",
       " 'VPIC6',\n",
       " 'comes',\n",
       " 'updated',\n",
       " 'every',\n",
       " 'card',\n",
       " 'level',\n",
       " 'Tseng4000',\n",
       " '24',\n",
       " 'bit',\n",
       " 'nice',\n",
       " 'affair',\n",
       " 'driver',\n",
       " 'Hope',\n",
       " 'useful',\n",
       " 'Bye',\n",
       " 'fulk',\n",
       " 'cs',\n",
       " 'rochester',\n",
       " 'Mark',\n",
       " 'Fulk',\n",
       " 'methodology',\n",
       " 'Homeopathy',\n",
       " 'tradition',\n",
       " 'Rochester',\n",
       " 'C5JE94',\n",
       " 'KrL',\n",
       " 'unx',\n",
       " 'sas',\n",
       " 'sasghm',\n",
       " 'theseus',\n",
       " 'Gary',\n",
       " 'Merrill',\n",
       " '1993Apr15',\n",
       " '161112',\n",
       " '21772',\n",
       " 'extra',\n",
       " 'scientific',\n",
       " 'phrase',\n",
       " 'discussion',\n",
       " 'boundaries',\n",
       " 'science',\n",
       " 'except',\n",
       " 'proposed',\n",
       " 'definiens',\n",
       " 'Extra',\n",
       " 'rational',\n",
       " 'better',\n",
       " 'number',\n",
       " 'considerations',\n",
       " 'direction',\n",
       " 'Yeah',\n",
       " 'holding',\n",
       " 'examples',\n",
       " 'exemplars',\n",
       " 'refutations',\n",
       " 'founded',\n",
       " 'smack',\n",
       " 'unuseful',\n",
       " 'directions',\n",
       " 'Lysenko',\n",
       " 'Such',\n",
       " 'curiosities',\n",
       " 'guides',\n",
       " 'noted',\n",
       " 'distinction',\n",
       " '_motivation_',\n",
       " '_method_',\n",
       " 'experimental',\n",
       " 'result',\n",
       " 'accepted',\n",
       " 'unless',\n",
       " 'described',\n",
       " 'sufficient',\n",
       " 'detail',\n",
       " 'replicated',\n",
       " 'replications',\n",
       " 'indeed',\n",
       " 'reproduce',\n",
       " 'theoretical',\n",
       " 'presented',\n",
       " 'followed',\n",
       " 'reasonable',\n",
       " 'knowlegeable',\n",
       " 'agree',\n",
       " 'force',\n",
       " 'logic',\n",
       " 'But',\n",
       " 'try',\n",
       " 'experiments',\n",
       " 'pursue',\n",
       " 'arguments',\n",
       " 'sorts',\n",
       " 'crazy',\n",
       " 'Irrational',\n",
       " 'motivations',\n",
       " 'curiousities',\n",
       " 'large',\n",
       " 'part',\n",
       " 'history',\n",
       " 'couple',\n",
       " 'negative',\n",
       " 'points',\n",
       " 'theory',\n",
       " 'qi',\n",
       " 'could',\n",
       " 'conceivably',\n",
       " 'become',\n",
       " 'without',\n",
       " 'direct',\n",
       " 'verification',\n",
       " 'existence',\n",
       " 'quarks',\n",
       " 'standard',\n",
       " 'model',\n",
       " 'physics',\n",
       " 'What',\n",
       " 'needed',\n",
       " 'based',\n",
       " 'predicted',\n",
       " 'reality',\n",
       " 'alternatives',\n",
       " 'central',\n",
       " 'claim',\n",
       " 'lie',\n",
       " 'forever',\n",
       " 'beyond',\n",
       " 'experiment',\n",
       " 'body',\n",
       " 'data',\n",
       " 'breath',\n",
       " 'waiting',\n",
       " 'triumph',\n",
       " 'though',\n",
       " 'even',\n",
       " 'coherent',\n",
       " 'less',\n",
       " 'explains',\n",
       " 'anything',\n",
       " 'modern',\n",
       " 'biology',\n",
       " 'hard',\n",
       " 'imagine',\n",
       " 'predict',\n",
       " 'rather',\n",
       " 'directly',\n",
       " 'verifying',\n",
       " 'historically',\n",
       " 'progressed',\n",
       " 'sort',\n",
       " 'sequence',\n",
       " 'carried',\n",
       " 'interpreted',\n",
       " 'pre',\n",
       " 'existing',\n",
       " 'frameworks',\n",
       " 'controversies',\n",
       " 'day',\n",
       " 'determine',\n",
       " 'done',\n",
       " 'Overall',\n",
       " 'huge',\n",
       " 'messy',\n",
       " 'personal',\n",
       " 'jealousies',\n",
       " 'petty',\n",
       " 'hatreds',\n",
       " 'determines',\n",
       " 'computations',\n",
       " 'going',\n",
       " 'forward',\n",
       " 'critical',\n",
       " 'function',\n",
       " 'results',\n",
       " 'count',\n",
       " 'whole',\n",
       " 'system',\n",
       " 'mechanism',\n",
       " 'generate',\n",
       " 'totally',\n",
       " 'irrational',\n",
       " 'properly',\n",
       " 'Pasteur',\n",
       " 'whatever',\n",
       " 'liked',\n",
       " 'chemical',\n",
       " 'activity',\n",
       " 'crystals',\n",
       " 'Mitscherlich',\n",
       " 'racemic',\n",
       " 'acid',\n",
       " 'handed',\n",
       " 'separate',\n",
       " 'handedness',\n",
       " 'chemicals',\n",
       " 'rotate',\n",
       " 'polarized',\n",
       " 'light',\n",
       " 'opposite',\n",
       " 'right',\n",
       " 'rotating',\n",
       " 'version',\n",
       " 'indistinguishable',\n",
       " 'tartaric',\n",
       " 'motivation',\n",
       " 'led',\n",
       " 'replicable',\n",
       " 'important',\n",
       " 'creationists',\n",
       " 'fail',\n",
       " 'usually',\n",
       " 'produced',\n",
       " 'theories',\n",
       " 'When',\n",
       " 'contradicted',\n",
       " 'concede',\n",
       " 'suppress',\n",
       " 'divert',\n",
       " 'attention',\n",
       " 'evidence',\n",
       " 'supports',\n",
       " 'position',\n",
       " 'Department',\n",
       " 'eylerken',\n",
       " 'stein',\n",
       " 'u',\n",
       " 'washington',\n",
       " 'Ken',\n",
       " 'Eyler',\n",
       " 'stand',\n",
       " 'alone',\n",
       " 'editing',\n",
       " 'suite',\n",
       " 'Article',\n",
       " 'D',\n",
       " 'shelley',\n",
       " '1qvkaeINNgat',\n",
       " 'Washington',\n",
       " 'Seattle',\n",
       " '12',\n",
       " 'need',\n",
       " 'We',\n",
       " 'upgrading',\n",
       " 'animation',\n",
       " 'video',\n",
       " 'different',\n",
       " 'type',\n",
       " 'setups',\n",
       " 'B',\n",
       " 'roll',\n",
       " 'cuts',\n",
       " 'station',\n",
       " 'controlled',\n",
       " 'computer',\n",
       " 'brand',\n",
       " 'doesnt',\n",
       " 'MAC',\n",
       " 'AMIGA',\n",
       " 'Low',\n",
       " 'high',\n",
       " 'helpful',\n",
       " 'might',\n",
       " 'mail',\n",
       " 'requirements',\n",
       " 'used',\n",
       " 'hardware',\n",
       " 'necessary',\n",
       " 'set',\n",
       " 'info',\n",
       " 'thanks',\n",
       " 'advance',\n",
       " 'ken',\n",
       " 'chris',\n",
       " 'sarah',\n",
       " 'lerc',\n",
       " 'nasa',\n",
       " 'Chris',\n",
       " 'Johnston',\n",
       " 'One',\n",
       " 'composites',\n",
       " 'seminar',\n",
       " 'NASA',\n",
       " 'Lewis',\n",
       " 'Center',\n",
       " 'Cleveland',\n",
       " 'OH',\n",
       " '47',\n",
       " 'Reply',\n",
       " 'To',\n",
       " 'looney',\n",
       " 'SAMPE',\n",
       " 'NCGA',\n",
       " 'Akron',\n",
       " 'sponsoring',\n",
       " 'COMPUTERS',\n",
       " 'AND',\n",
       " 'COMPOSITES',\n",
       " 'devoted',\n",
       " 'practical',\n",
       " 'applications',\n",
       " 'workstations',\n",
       " 'efficient',\n",
       " 'design',\n",
       " 'Manufacture',\n",
       " 'May',\n",
       " '1993',\n",
       " 'Ohio',\n",
       " 'Speakers',\n",
       " 'Advancement',\n",
       " 'Graphics',\n",
       " 'Visualization',\n",
       " 'Jay',\n",
       " 'Horowitz',\n",
       " 'Integrated',\n",
       " 'Product',\n",
       " 'Development',\n",
       " 'Mr',\n",
       " 'Michael',\n",
       " 'R',\n",
       " 'Cowen',\n",
       " 'Network',\n",
       " 'Workstations',\n",
       " 'Sikorski',\n",
       " 'Aircraft',\n",
       " 'Structural',\n",
       " 'Analysis',\n",
       " 'Brian',\n",
       " 'Fite',\n",
       " 'Stereolithography',\n",
       " 'Jason',\n",
       " 'Williams',\n",
       " 'Penn',\n",
       " 'Erie',\n",
       " 'Molecular',\n",
       " 'Physical',\n",
       " 'Modeling',\n",
       " 'Vassilios',\n",
       " 'Galiatsato',\n",
       " 'Polymer',\n",
       " 'Curing',\n",
       " 'Process',\n",
       " 'Matrix',\n",
       " 'Composites',\n",
       " 'Upadhyay',\n",
       " 'GE',\n",
       " 'Corporate',\n",
       " 'Registration',\n",
       " 'Fees',\n",
       " '75',\n",
       " '00',\n",
       " 'site',\n",
       " ...]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Firstly counting the number of unique words\n",
    "unique_words_list = []\n",
    "for news in dataset_data_no_stopwords :\n",
    "    for word in news :\n",
    "        if word not in unique_words_list :\n",
    "            unique_words_list.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constructing bag-of-words matrix representation for each sample\n",
    "#Initializing with zeros before counting\n",
    "dataset_bag_of_words = np.zeros((len(dataset_data_no_stopwords), len(unique_words_list)))\n",
    "\n",
    "for news_index in range(len(dataset_data_no_stopwords)) :\n",
    "    for word_index in range(len(unique_words_list)) :\n",
    "        dataset_bag_of_words[news_index, word_index] = dataset_bag_of_words[news_index, word_index] + dataset_data_no_stopwords[news_index].count(unique_words_list[word_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Implementing a TF-IDF feature representation for each text sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part I use the following formula for idf : idf= -log(num_documents_per_word / total_num_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating TF-IDF value for each item in the bag of words\n",
    "#Number of samples for idf calculations\n",
    "N = dataset_bag_of_words.shape[0]\n",
    "num_documents_per_word = np.zeros((dataset_bag_of_words.shape[1],1))\n",
    "dataset_tf_idf = np.zeros(dataset_bag_of_words.shape)\n",
    "for news_index in range(dataset_bag_of_words.shape[0]) :\n",
    "    #Making sure that expensive operations occurs once per iteration\n",
    "    words_count = np.sum(dataset_bag_of_words[news_index, :])\n",
    "    for word_index in range(dataset_bag_of_words.shape[1]) :\n",
    "        if(num_documents_per_word[word_index] == 0) :\n",
    "            num_documents_per_word[word_index] = np.count_nonzero(dataset_bag_of_words[:,word_index])\n",
    "        #calculating tf by dividing count over total number of words per document\n",
    "        tf = dataset_bag_of_words[news_index, word_index] / words_count\n",
    "        #calculaing idf as -log(num_documents_for_word/total_num_documents)\n",
    "        #Preventing -0.0 values\n",
    "        if(num_documents_per_word[word_index] == N) :\n",
    "            idf = 0\n",
    "        else :\n",
    "            idf = -math.log(num_documents_per_word[word_index]/N)\n",
    "        dataset_tf_idf[news_index, word_index] = tf*idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Split the dataset randomly into train/validation/test splits according to ratios 80%:10%:10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, rest_data, train_target, rest_target = train_test_split(dataset_tf_idf, dataset_target, test_size=0.2, random_state=3116)\n",
    "val_data, test_data, val_target, test_target = train_test_split(rest_data, rest_target, test_size=0.5, random_state=3116)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1: Implementing Naive Bayes Classifier for Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise all features are numerical so a normal distribution with mean and standard deviation for each feature with each class is calculated in the training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_means(train_data, train_target) :\n",
    "    classes = np.unique(train_target)\n",
    "    mean_matrix = np.zeros((classes.shape[0], train_data.shape[1]))\n",
    "    for class_index in range(classes.shape[0]) :\n",
    "        current_class_data = train_data[train_target==classes[class_index]]\n",
    "        for feature_index in range(train_data.shape[1]) :\n",
    "            #Getting the mean of a certain class with a certain feature \n",
    "            mean_matrix[class_index, feature_index] = np.sum(current_class_data[:,feature_index]) / current_class_data.shape[0]\n",
    "    return mean_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stds(train_data, train_target, mean_matrix) :\n",
    "    classes = np.unique(train_target)\n",
    "    std_matrix = np.zeros((classes.shape[0], train_data.shape[1]))\n",
    "    for class_index in range(classes.shape[0]) :\n",
    "        current_class_data = train_data[train_target==classes[class_index]]\n",
    "        for feature_index in range(train_data.shape[1]) :\n",
    "            #Getting the mean of a certain class with a certain feature \n",
    "            mean_vector = np.ones(train_data[:,feature_index].shape)*mean_matrix[class_index, feature_index]\n",
    "            std_matrix[class_index, feature_index] = np.sqrt(np.sum(np.square(np.subtract(train_data[:, feature_index], \n",
    "                                                                                  mean_vector))) / current_class_data.shape[0])\n",
    "    return std_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classes_probabilities(train_target, classes) :\n",
    "    classes_probabilities = np.zeros(classes.shape[0])\n",
    "    for class_index in range(classes.shape[0]) :\n",
    "        classes_probabilities[class_index] = train_target[train_target==classes[class_index]].shape[0]/train_target.shape[0]\n",
    "    return classes_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_naive_bayes(train_data, train_target) :\n",
    "    mean_matrix = calculate_means(train_data, train_target)\n",
    "    std_matrix = calculate_stds(train_data, train_target, mean_matrix)\n",
    "    classes = np.unique(train_target)\n",
    "    classes_probabilities = calculate_classes_probabilities(train_target, classes)\n",
    "    return mean_matrix, std_matrix, classes, classes_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the testing phase we use the mean and standard deviation to approximate the probability of the new instance according to the normal distribution then the class with the highest porbability is the class label for this instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding epsilon here to prevent devision by zero\n",
    "def calculate_probability(X, mean, std, epsilon=0.00003) :\n",
    "    first_term = 1/((std+epsilon)*np.sqrt(2*math.pi))\n",
    "    second_term = np.exp(-0.5*((X-mean)/(std+epsilon))**2)\n",
    "    probability = first_term*second_term\n",
    "    #print(X, \" \", mean, \" \", std)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating prediction for naive bayes\n",
    "def predict_naive_bayes(test_data, test_target, mean_matrix, std_matrix, classes, classes_probabilities) :\n",
    "    predictions = np.zeros(test_target.shape)\n",
    "    for test_instance_index in range(test_data.shape[0]) :\n",
    "        test_instance = test_data[test_instance_index]\n",
    "        current_instance_probabilities = np.zeros(classes.shape[0])\n",
    "        for class_index in range(classes.shape[0]) :\n",
    "            current_instance_probabilities[class_index] = math.log(classes_probabilities[class_index])\n",
    "            for feature_index in range(test_data.shape[1]) : \n",
    "                current_feature_class_prob = calculate_probability(test_instance[feature_index], mean_matrix[class_index, feature_index], std_matrix[class_index, feature_index])\n",
    "                if(current_feature_class_prob == 0) :\n",
    "                    current_log_probability = 0\n",
    "                else :\n",
    "                    current_log_probability = math.log(current_feature_class_prob)\n",
    "                current_instance_probabilities[class_index] = current_instance_probabilities[class_index]+current_log_probability\n",
    "        predictions[test_instance_index] = classes[np.argmax(current_instance_probabilities)]\n",
    "        #print(current_instance_probabilities)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_naive_bayes(predictions, test_target) :\n",
    "    return (np.sum(predictions==test_target) / test_target.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing some samples to make the data balanced for the naive bayes classifier to work well\n",
    "difference = train_target[train_target == 13].shape[0] - train_target[train_target==1].shape[0]\n",
    "train_data_balanced = train_data[train_target==13][:-difference].copy()\n",
    "train_target_balanced = train_target[train_target==13][:-difference].copy()\n",
    "train_data_balanced = np.append(train_data_balanced, train_data[train_target==1], axis=0)\n",
    "train_target_balanced = np.append(train_target_balanced, train_target[train_target==1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_matrix, std_matrix, classes, classes_probabilities = learn_naive_bayes(train_data_balanced, train_target_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive bayes validation accuracy is  95.40816326530613\n"
     ]
    }
   ],
   "source": [
    "validation_prediction = predict_naive_bayes(val_data, val_target, mean_matrix, std_matrix, classes, classes_probabilities)\n",
    "print(\"Naive bayes validation accuracy is \", score_naive_bayes(validation_prediction, val_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive bayes test accuracy is  94.9238578680203\n"
     ]
    }
   ],
   "source": [
    "test_prediction = predict_naive_bayes(test_data, test_target, mean_matrix, std_matrix, classes, classes_probabilities)\n",
    "print(\"Naive bayes test accuracy is \", score_naive_bayes(test_prediction, test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn Naive bayes accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(train_data_balanced, train_target_balanced).predict(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn naive bayes test accuracy is  95.43147208121827\n"
     ]
    }
   ],
   "source": [
    "print(\"Sklearn naive bayes test accuracy is \", score_naive_bayes(y_pred, test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2: Implementing SVM Classifier via Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_accuracy is  84.6938775510204 with penalty type  l1  and C is  0.5\n",
      "current_accuracy is  88.26530612244898 with penalty type  l1  and C is  1\n",
      "current_accuracy is  90.81632653061224 with penalty type  l1  and C is  2\n",
      "current_accuracy is  91.83673469387756 with penalty type  l1  and C is  10\n",
      "current_accuracy is  97.44897959183673 with penalty type  l2  and C is  0.5\n",
      "current_accuracy is  97.44897959183673 with penalty type  l2  and C is  1\n",
      "current_accuracy is  97.44897959183673 with penalty type  l2  and C is  2\n",
      "current_accuracy is  97.44897959183673 with penalty type  l2  and C is  10\n"
     ]
    }
   ],
   "source": [
    "#Validating different SVM kernels with different hyperparameters\n",
    "penalty_linear_SVC = ['l1', 'l2']\n",
    "C_parameters = [0.5, 1, 2, 10]\n",
    "\n",
    "best_accuracy_linear_svm = 0\n",
    "best_hyperparameters_linear_svm = {'penalty type' : 'l1', 'C parameter' : 0.5}\n",
    "\n",
    "#Validation of linear SVM\n",
    "for penalty_type in penalty_linear_SVC :\n",
    "    for C_parameter in C_parameters :\n",
    "        if(penalty_type == 'l1') :\n",
    "            model = LinearSVC(penalty=penalty_type, dual=False, C=C_parameter)\n",
    "        else :\n",
    "            model = LinearSVC(penalty=penalty_type, C=C_parameter)\n",
    "        model.fit(train_data, train_target)\n",
    "        current_model_score = 100*model.score(val_data, val_target)\n",
    "        if(current_model_score > best_accuracy_linear_svm) :\n",
    "            best_accuracy_linear_svm = current_model_score\n",
    "            best_hyperparameters_linear_svm = {'penalty_type' : penalty_type, 'C parameter' : C_parameter}\n",
    "        print('current_accuracy is ', current_model_score, 'with penalty type ', penalty_type, ' and C is ', C_parameter)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy for SVM is  97.44897959183673  with paramters :  {'penalty_type': 'l2', 'C parameter': 0.5}\n"
     ]
    }
   ],
   "source": [
    "print('Best validation accuracy for SVM is ', best_accuracy_linear_svm, ' with paramters : ', best_hyperparameters_linear_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current accuracy us  54.59183673469388  with hyperparameters  {'degree': 2, 'gamma': 'scale', 'C parameter': 0.5}\n",
      "Current accuracy us  66.3265306122449  with hyperparameters  {'degree': 2, 'gamma': 'scale', 'C parameter': 1}\n",
      "Current accuracy us  72.95918367346938  with hyperparameters  {'degree': 2, 'gamma': 'scale', 'C parameter': 2}\n",
      "Current accuracy us  47.44897959183674  with hyperparameters  {'degree': 2, 'gamma': 'auto', 'C parameter': 0.5}\n",
      "Current accuracy us  47.44897959183674  with hyperparameters  {'degree': 2, 'gamma': 'auto', 'C parameter': 1}\n",
      "Current accuracy us  47.44897959183674  with hyperparameters  {'degree': 2, 'gamma': 'auto', 'C parameter': 2}\n",
      "Current accuracy us  49.48979591836735  with hyperparameters  {'degree': 3, 'gamma': 'scale', 'C parameter': 0.5}\n",
      "Current accuracy us  50.51020408163265  with hyperparameters  {'degree': 3, 'gamma': 'scale', 'C parameter': 1}\n",
      "Current accuracy us  52.55102040816326  with hyperparameters  {'degree': 3, 'gamma': 'scale', 'C parameter': 2}\n",
      "Current accuracy us  47.44897959183674  with hyperparameters  {'degree': 3, 'gamma': 'auto', 'C parameter': 0.5}\n",
      "Current accuracy us  47.44897959183674  with hyperparameters  {'degree': 3, 'gamma': 'auto', 'C parameter': 1}\n",
      "Current accuracy us  47.44897959183674  with hyperparameters  {'degree': 3, 'gamma': 'auto', 'C parameter': 2}\n"
     ]
    }
   ],
   "source": [
    "#Trying different values with the validation set for polynomial kernel\n",
    "poly_svm_degrees = [2,3]\n",
    "gamma_values = ['scale', 'auto']\n",
    "C_values = [0.5, 1, 2]\n",
    "\n",
    "best_accuracy_poly_svm = 0\n",
    "best_hyperparameters_poly_svm = {'degree' : 2, 'gamma' : 'scale', 'C parameter' : 0.5}\n",
    "\n",
    "for poly_svm_degree in poly_svm_degrees :\n",
    "    for gamma_value in gamma_values :\n",
    "        for C_value in C_values :\n",
    "            model = SVC(kernel='poly', degree=poly_svm_degree, cache_size=6000, gamma=gamma_value, C=C_value)\n",
    "            model.fit(train_data, train_target)\n",
    "            current_score = 100*model.score(val_data, val_target)\n",
    "            current_hyperparameters = {'degree' : poly_svm_degree, 'gamma' : gamma_value, 'C parameter' : C_value}\n",
    "            if(current_score > best_accuracy_poly_svm) :\n",
    "                best_accuracy_poly_svm = current_score\n",
    "                best_hyperparameters_poly_svm = current_hyperparameters\n",
    "            print('Current accuracy us ', current_score, ' with hyperparameters ', current_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy for polynomial SVM is  72.95918367346938  with paramters :  {'degree': 2, 'gamma': 'scale', 'C parameter': 2}\n"
     ]
    }
   ],
   "source": [
    "print('Best validation accuracy for polynomial SVM is ', best_accuracy_poly_svm, ' with paramters : ', best_hyperparameters_poly_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current accuracy us  88.26530612244898  with hyperparameters  {'kernel': 'rbf', 'gamma': 'scale', 'C parameter': 0.5}\n",
      "Current accuracy us  47.44897959183674  with hyperparameters  {'kernel': 'rbf', 'gamma': 'auto', 'C parameter': 0.5}\n",
      "Current accuracy us  91.83673469387756  with hyperparameters  {'kernel': 'rbf', 'gamma': 'scale', 'C parameter': 1}\n",
      "Current accuracy us  47.44897959183674  with hyperparameters  {'kernel': 'rbf', 'gamma': 'auto', 'C parameter': 1}\n",
      "Current accuracy us  92.85714285714286  with hyperparameters  {'kernel': 'rbf', 'gamma': 'scale', 'C parameter': 2}\n",
      "Current accuracy us  47.44897959183674  with hyperparameters  {'kernel': 'rbf', 'gamma': 'auto', 'C parameter': 2}\n",
      "Current accuracy us  97.44897959183673  with hyperparameters  {'kernel': 'sigmoid', 'gamma': 'scale', 'C parameter': 0.5}\n",
      "Current accuracy us  47.44897959183674  with hyperparameters  {'kernel': 'sigmoid', 'gamma': 'auto', 'C parameter': 0.5}\n",
      "Current accuracy us  97.44897959183673  with hyperparameters  {'kernel': 'sigmoid', 'gamma': 'scale', 'C parameter': 1}\n",
      "Current accuracy us  47.44897959183674  with hyperparameters  {'kernel': 'sigmoid', 'gamma': 'auto', 'C parameter': 1}\n",
      "Current accuracy us  96.93877551020408  with hyperparameters  {'kernel': 'sigmoid', 'gamma': 'scale', 'C parameter': 2}\n",
      "Current accuracy us  47.44897959183674  with hyperparameters  {'kernel': 'sigmoid', 'gamma': 'auto', 'C parameter': 2}\n"
     ]
    }
   ],
   "source": [
    "#Measuring accuracies for rbf and sigmoid kernels with different C parameters\n",
    "kernels = ['rbf', 'sigmoid']\n",
    "C_values = [0.5, 1, 2]\n",
    "gamma_values = ['scale', 'auto']\n",
    "\n",
    "best_accuracy_other_kernels_svm = 0\n",
    "best_hyperparameters_other_kernels_svm = {'kernel' : 'rbf', 'gamma' : 'scale', 'C parameter' : 0.5}\n",
    "\n",
    "\n",
    "for kernel in kernels :\n",
    "    for C_value in C_values :\n",
    "        for gamma in gamma_values :\n",
    "            model = SVC(kernel=kernel, gamma=gamma, cache_size=6000, C=C_value)\n",
    "            model.fit(train_data, train_target)\n",
    "            current_score = 100*model.score(val_data, val_target)\n",
    "            current_hyperparameters = {'kernel' : kernel, 'gamma' : gamma, 'C parameter' : C_value}\n",
    "            if(current_score > best_accuracy_other_kernels_svm) :\n",
    "                best_accuracy_other_kernels_svm = current_score\n",
    "                best_hyperparameters_other_kernels_svm = current_hyperparameters\n",
    "            print('Current accuracy us ', current_score, ' with hyperparameters ', current_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy for other kernels SVM is  97.44897959183673  with paramters :  {'kernel': 'sigmoid', 'gamma': 'scale', 'C parameter': 0.5}\n"
     ]
    }
   ],
   "source": [
    "print('Best validation accuracy for other kernels SVM is ', best_accuracy_other_kernels_svm, ' with paramters : ', best_hyperparameters_other_kernels_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that linear and sigmoid kernels achieve best validation accuracy of 97.44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Report the test-set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with best linear svm is  98.47715736040608\n"
     ]
    }
   ],
   "source": [
    "#Testing on best hyperparameters with linear\n",
    "linear_model = LinearSVC(penalty='l2', C=0.5)\n",
    "linear_model.fit(train_data, train_target)\n",
    "print('Test accuracy with best linear svm is ', linear_model.score(test_data, test_target)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with best sigmoid kernel svm is  98.47715736040608\n"
     ]
    }
   ],
   "source": [
    "#Testing on best hyperparameters with sigmoid kernel\n",
    "sigmoid_kernel_model = SVC(kernel='sigmoid', gamma='scale', C=0.5)\n",
    "sigmoid_kernel_model.fit(train_data, train_target)\n",
    "print('Test accuracy with best sigmoid kernel svm is ', sigmoid_kernel_model.score(test_data, test_target)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can see sigmoid and linear kernels give similar validation and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
